# Olist â€“ de CSV bruts Ã  base analytique 

Un README pour prÃ©senter un pipeline ETL simple et fiable, du dataset Olist jusquâ€™Ã  une base analytique en Ã©toile et des KPI mÃ©tiers actionnables.

---

## ğŸ¯ **Objectif :** transformer les CSV bruts Olist en une base SQLite modÃ©lisÃ©e en schÃ©ma en Ã©toile pour rÃ©pondre Ã  des questions mÃ©tier (ventes, logistique, satisfaction, paiements).

**Approche :** 3 Ã©tapes Exploration â†’ Transformation/Chargement â†’ Analyse ; nettoyage traÃ§able, rÃ¨gles qualitÃ© explicites, zÃ©ro suppression destructive.

**Sorties :** tables propres, fact_order_items + dimensions, requÃªtes SQL KPI, et une narration claire pour comprendre le pourquoi derriÃ¨re le comment.

---

## 1) Pourquoi ce projet ? 

Quand on ouvre pour la premiÃ¨re fois les fichiers Olist, on voit des CSV hÃ©tÃ©rogÃ¨nes, des dates parfois manquantes et des tables qui se rÃ©pondent sans toujours sâ€™aligner. Mon dÃ©fi a Ã©tÃ© de rassembler, nettoyer et modÃ©liser ces donnÃ©es de maniÃ¨re reproductible, puis dâ€™en tirer des indicateurs mÃ©tier qui aident Ã  dÃ©cider : oÃ¹ Ã§a marche ? oÃ¹ Ã§a coince ? et que peut-on amÃ©liorer ?

---

## 2) Le dataset en deux mots

Le projet sâ€™appuie sur 9 fichiers dÃ©crivant lâ€™Ã©cosystÃ¨me e-commerce Olist (clients, commandes, articles, paiements, avis, produits, vendeurs, gÃ©olocalisation, traduction des catÃ©gories) sur la pÃ©riode 2016â€“2018. Lâ€™objectif est de passer dâ€™un ensemble de fichiers plats Ã  une base analytique interrogeable efficacement.

---

## 3) Ma stratÃ©gie dâ€™analyse (les 4 piliers)

**Performance Commerciale** â€“ comprendre lâ€™Ã©volution des ventes, la saisonnalitÃ©, les catÃ©gories et vendeurs qui tirent le chiffre dâ€™affaires.

**EfficacitÃ© Logistique** â€“ comparer dÃ©lais rÃ©els vs estimÃ©s, repÃ©rer les Ã‰tats/rÃ©gions en difficultÃ© et la rÃ©activitÃ© des vendeurs.

**Satisfaction Client** â€“ analyser les review_score, les motifs dâ€™insatisfaction (souvent liÃ©s Ã  la livraison) et lâ€™impact des retards sur les notes.

**Comportement dâ€™Achat** â€“ Ã©tudier les modes de paiement (carte, boletoâ€¦) et lâ€™usage du paiement Ã©chelonnÃ© (ex. 6x, 10x) et son effet sur le panier moyen.

---

## 4) Architecture du pipeline (ETL)

Trois blocs simples, pensÃ©s pour une premiÃ¨re itÃ©ration reproductible :

- **01_exploration.ipynb** â€“ audit de qualitÃ© : typage, valeurs manquantes, doublons, cohÃ©rence des clÃ©s et chronologie des dates.
- **02_pipeline.ipynb** â€“ transformations : typage explicite (dates, numÃ©riques, identifiants), dÃ©doublonnage de la gÃ©olocalisation, version canonique des avis (dÃ©duplication par review_id), chargement modÃ©lisÃ© dans SQLite via SQLAlchemy.
- **03_requetes.ipynb** â€“ requÃªtes dâ€™analyse et KPI sur la base finale.

**Technos clÃ©s :** Python, pandas, SQLAlchemy, SQLite, notebooks Jupyter.

---

## 5) RÃ¨gles de nettoyage & qualitÃ© (non destructives)

- **Dates manquantes = Ã©tat mÃ©tier** : on ne remplit pas artificiellement une livraison non encore effectuÃ©e ; on conserve NULL et on flague.
- **IntÃ©gritÃ© rÃ©fÃ©rentielle** : vÃ©rification systÃ©matique des FK entre faits et dimensions.
- **UnicitÃ©** : contrainte dâ€™unicitÃ© sur le couple (order_id, order_item_id) pour bannir les doublons techniques.
- **Chronologie (soft-checks)** : purchase â‰¤ approved â‰¤ delivered_carrier â‰¤ delivered_customer â‰¤ estimated_delivery ; en cas dâ€™Ã©cart, on flague et on exclut des KPI de dÃ©lais sans altÃ©rer la donnÃ©e brute.
- **TraÃ§abilitÃ©** : colonnes *_missing, *_imputed, qc_* ; jobs idempotents (UPSERT/MERGE logiques).

---

## 6) ModÃ©lisation : schÃ©ma en Ã©toile

La base `olist_final.sqlite` est structurÃ©e pour les requÃªtes analytiques rapides.

**SchÃ©ma visuel clair du pipeline (BSG)**
- Diagramme du pipeline Bronze â†’ Silver â†’ Gold (+ SQLite) 

flowchart LR
    subgraph Bronze
      A[data/bronze<br/>Raw CSVs] --> B["src/extract.py<br/>load_all()"]
    end

    subgraph Silver
      C["src/transform.py<br/>cast_basic_types()<br/>add_quality_flags()<br/>reviews_canonical()<br/>geolocation_dedup()"] --> D[data/silver<br/>Clean CSVs]
    end

    subgraph Gold
      E[src/model.py<br/>Dims + Fact] --> F["src/load.py<br/>apply_schema()<br/>load_tables()"]
      F --> G["data/db/olist.db<br/>SQLite (star schema)"]
    end

    B --> C
    D --> E

    H["[notebooks<br/>01_exploration<br/>02_etl<br/>03_analytics]"]


### Table de faits

**fact_order_items** â€” grain : 1 ligne = 1 produit vendu dans une commande.  
Mesures : price, freight_value (et mÃ©triques dÃ©rivÃ©es).

### Tables de dimensions

- dim_customers â€” localisation & identifiants clients
- dim_products â€” attributs produits & catÃ©gories
- dim_sellers â€” informations vendeurs
- dim_date â€” calendrier (jour, semaine, mois, trimestre, annÃ©e)

       dim_date            dim_customers          dim_products         dim_sellers
           â”‚                     â”‚                       â”‚                   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
                          â–¼                     â–¼                        â–¼
                    fact_order_items (price, freight_value, â€¦)


---

## 7) Des KPIâ€¦ au service dâ€™une histoire

Au-delÃ  des chiffres, je raconte lâ€™Ã©volution de la marketplace :

- **Ventes & saisonnalitÃ©** : commandes par mois, CA mensuel, panier moyen ; oÃ¹ sont les pics ? qui les porte ?
- **Top performances** : top catÃ©gories et meilleurs vendeurs ; qui tire le marchÃ© ?
- **Logistique** : dÃ©lai moyen de livraison, retard moyen et taux de retard ; quelles rÃ©gions souffrent ?
- **Satisfaction** : distribution des review_score et impact des retards sur les notes ; oÃ¹ lâ€™expÃ©rience casse ?
- **Paiement** : part des cartes vs boleto, usage du paiement en plusieurs fois et effet sur le panier moyen.

### Exemples de requÃªtes (extraits)

```sql
-- Commandes par mois
SELECT strftime('%Y-%m', o.order_purchase_timestamp) AS mois,
       COUNT(DISTINCT o.order_id) AS nb_commandes
FROM orders o
GROUP BY 1
ORDER BY 1;


-- DÃ©lai rÃ©el vs estimÃ© (en jours)
SELECT c.customer_state AS etat,
       AVG(julianday(o.order_delivered_customer_date) - julianday(o.order_purchase_timestamp)) AS delai_reel_j,
       AVG(julianday(o.order_estimated_delivery_date) - julianday(o.order_purchase_timestamp)) AS delai_estime_j
FROM orders o
JOIN customers c ON c.customer_id = o.customer_id
WHERE o.order_delivered_customer_date IS NOT NULL
GROUP BY 1
ORDER BY 2 DESC;

8) Prise en main (exÃ©cution rapide)

# 1) CrÃ©er et activer un venv
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# 2) Installer les dÃ©pendances minimales
pip install pandas sqlalchemy jupyter

# 3) Organiser les donnÃ©es
# data/raw/  -> CSV bruts Olist
# data/processed/ -> sorties nettoyÃ©es

# 4) Lancer les notebooks dans lâ€™ordre
# 01_exploration.ipynb -> 02_pipeline.ipynb -> 03_requetes.ipynb


9) Structure du repository

olist-data-cleaning/
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # CSV bruts (non versionnÃ©s)
â”‚   â””â”€â”€ processed/           # CSV nettoyÃ©s (non versionnÃ©s)
â”œâ”€â”€ db/                      # SQLite (non versionnÃ©)
â”‚   â””â”€â”€ olist_final.sqlite
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ README.md            # index documentation
â”‚   â”œâ”€â”€ note_cadrage.md
â”‚   â”œâ”€â”€ regles_nettoyage.md
â”‚   â”œâ”€â”€ dictionnaire_data.md
â”‚   â””â”€â”€ schema_etoile.png
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploration.ipynb
â”‚   â”œâ”€â”€ 02_pipeline.ipynb
â”‚   â””â”€â”€ 03_kpi.ipynb
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ ddl/
â”‚   â”‚   â””â”€â”€ schema_etoile.sql
â”‚   â”œâ”€â”€ kpi/
â”‚   â”‚   â”œâ”€â”€ 01_commandes_mensuelles.sql
â”‚   â”‚   â”œâ”€â”€ 02_CA_mensuel.sql
â”‚   â”‚   â”œâ”€â”€ 03_panier_moyen.sql
â”‚   â”‚   â”œâ”€â”€ 04_top_categories.sql
â”‚   â”‚   â”œâ”€â”€ 05_top_vendeurs.sql
â”‚   â”‚   â”œâ”€â”€ 06_repartition_geographique_clients.sql
â”‚   â”‚   â”œâ”€â”€ 07_delai_moyen_livraison.sql
â”‚   â”‚   â”œâ”€â”€ 08_taux_livraisons_en_retard.sql
â”‚   â”‚   â””â”€â”€ 09_retard_moyen_livraison.sql
â”‚   â””â”€â”€ checks/
â”‚       â”œâ”€â”€ pk_uniqueness.sql
â”‚       â””â”€â”€ fk_integrity.sql
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ config.py
    â”œâ”€â”€ db.py                # connexion SQLite/SQLAlchemy
    â”œâ”€â”€ etl/
    â”‚   â”œâ”€â”€ extract.py        # lecture raw
    â”‚   â”œâ”€â”€ transform.py      # typage + rules + qc flags
    â”‚   â””â”€â”€ load.py           # insert into SQLite
    â””â”€â”€ quality/
        â”œâ”€â”€ checks.py         # contrÃ´les PK/FK/temporal
        â””â”€â”€ report.py         # rÃ©sumÃ© des qc flags



10) Limites & prochaines Ã©tapes

Ã‰tendre le modÃ¨le avec une fact_orders dÃ©diÃ©e Ã  la logistique
Enrichir la dimension produits
Ajouter des tests de donnÃ©es (Great Expectations / dbt tests)
Publier des dashboards BI
Automatiser via Airflow

11) Auteure

Projet pÃ©dagogique rÃ©alisÃ© en mode data engineering : nettoyage â†’ gouvernance â†’ modÃ©lisation â†’ KPI â†’ storytelling.

â€” Sabine ANOKO